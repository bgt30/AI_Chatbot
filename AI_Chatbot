import os
from getpass import getpass
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.document_loaders import PyPDFLoader
import faiss

# Step 1: Set up environment
os.environ["OPENAI_API_KEY"] = getpass("OpenAI API key 입력: ")

# Step 2: Load the model
model = ChatOpenAI(model="gpt-4o-mini")

# Step 3: Load the document
pdf_loader = PyPDFLoader("~/desktop/Chatbot/인공지능산업최신동향_2024년11월호.pdf")
docs = pdf_loader.load()

# Step 4: Split the document into chunks using both methods
# i. Character Text Splitter
character_text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=100,
    chunk_overlap=10,
    length_function=len,
    is_separator_regex=False,
)
character_splits = character_text_splitter.split_documents(docs)

# ii. Recursive Character Text Splitter
recursive_text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=10,
    length_function=len,
    is_separator_regex=False,
)
recursive_splits = recursive_text_splitter.split_documents(docs)

# Print the first 10 chunks of each splitting method for visual inspection
print("CharacterTextSplitter Result:\n", character_splits[:10])
print("\nRecursiveCharacterTextSplitter Result:\n", recursive_splits[:10])

# Markdown Summary of Text Splitters
print("\n**CharacterTextSplitter**: Splits documents by a character separator. Parameters include separator, chunk size, and chunk overlap.\n")
print("**RecursiveCharacterTextSplitter**: Similar to CharacterTextSplitter but allows for more flexibility and efficiency in dealing with different chunking needs.\n")

# Use CharacterTextSplitter results for next steps
splits = character_splits

# Step 5: Generate vector embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# Step 6: Create the FAISS vector store
vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

# Step 7: Convert FAISS to Retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 1})

# Step 8: Define the prompt template
contextual_prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the question using only the following context."),
    ("user", "Context: {context}\n\nQuestion: {question}")
])

# Step 9: Create RAG Chain with debugging
class DebugPassThrough(RunnablePassthrough):
    def invoke(self, *args, **kwargs):
        output = super().invoke(*args, **kwargs)
        print("Debug Output:", output)
        return output

class ContextToText(RunnablePassthrough):
    def invoke(self, inputs, config=None, **kwargs):
        context_text = "\n".join([doc.page_content for doc in inputs["context"]])
        return {"context": context_text, "question": inputs["question"]}

rag_chain_debug = {
    "context": retriever,
    "question": DebugPassThrough()
} | DebugPassThrough() | ContextToText() | contextual_prompt | model

# Step 10: Run chatbot and compare with general LLM response
general_model = ChatOpenAI(model="gpt-4o-mini")

while True:
    print("========================")
    query = input("질문을 입력하세요 (종료하려면 '종료' 입력): ")
    if query.lower() == '종료':
        print("챗봇을 종료합니다.")
        break
    # Get response from RAG chain
    response = rag_chain_debug.invoke(query)
    print("Final Response from RAG Chain:")
    print(response.content)
    
    # Get response from general LLM without context retrieval
    general_response = general_model([HumanMessage(content=query)])
    print("\nGeneral LLM Response:")
    print(general_response.content)

# Why RAG is needed:
print("\n**Why RAG is Needed**: Retrieval-Augmented Generation (RAG) is important because it allows the model to use external knowledge to answer questions accurately, leveraging the context from relevant documents. It ensures that the response is based on real, up-to-date information, rather than relying solely on pre-trained data.")

# 영어 공부도 함께 하는 중이라 주석을 영어로 달았습니다..!